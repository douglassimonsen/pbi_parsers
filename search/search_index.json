{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Based on Crafting Interpreters. Library provides lexers, parsers, and formatters for DAX and Power Query (M) languages. Designed to support code introspection and analysis, not execution. This enables development of ruff-equivalent tools for DAX and Power Query. It also enables extracting metadata from DAX and Power Query code, such PQ source types (Excel, SQL, etc.) and DAX lineage dependencies.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>python -m pip install pbi_parsers\n</code></pre>"},{"location":"#functionality","title":"Functionality","text":"<p>Rust Implementation</p> <p>Although the library is primarily implemented in Python, there are plans to implement a Rust version for performance and additional type-safety.</p> <ul> <li>DAX<ul> <li> Lexer</li> <li> Parser</li> <li> Formatter</li> <li> Testing</li> <li> Rust Implementation</li> </ul> </li> <li>Power Query (M)<ul> <li> Lexer</li> <li> Parser</li> <li> Formatter</li> <li> Testing</li> <li> Rust Implementation</li> </ul> </li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>Formatting DAX Expressions</p> <p>Like <code>ruff</code> for Python, this library can format DAX expressions to improve readability and maintainability.</p> <pre><code>from pbi_parsers.dax import format_expression\n\ninput_dax = \"\"\"\nfunc.name(arg1 + 1 + 2  + 3, func(), func(10000000000000), arg2)\n\"\"\"\nformatted_dax = format_expression(input_dax)\nprint(formatted_dax)\n# Output:\n# func.name(\n#     arg1 + 1 + 2 + 3,\n#     func(),\n#     func(10000000000000),\n#     arg2\n# )\n</code></pre> <p>Creating AST Trees from DAX Expressions</p> <p>The library can parse DAX expressions into Abstract Syntax Trees (ASTs) for further analysis or manipulation.</p> <pre><code>from pbi_parsers.dax import to_ast\n\ninput_dax = \"\"\"\nfunc.name(arg1 + 1 + 2  + 3, func(), func(10000000000000), arg2)\n\"\"\"\nast = to_ast(input_dax)\nprint(ast)\n# Output: \n# Function (\n#     name: func.name,\n#     args: Add (\n#               left: Identifier (arg1),\n#               right: Add (\n#                          left: Number (1),\n#                          right: Add (\n#                                     left: Number (2),\n#                                     right: Number (3)\n#                                 )\n#                      )\n#           ),\n#           Function (\n#               name: func,\n#               args:\n#           ),\n#           Function (\n#               name: func,\n#               args: Number (10000000000000)\n#           ),\n#           Identifier (arg2)\n# )\n</code></pre> <p>Highlighting DAX Sections</p> <p>The library can highlight sections of DAX code, making it easier to identify and analyze specific parts of the code.</p> <p>Note: in the console, the caret (<code>^</code>) will be yellow and the line number will be cyan.</p> <pre><code>from pbi_parsers.dax import highlight_section, to_ast\n\ninput_dax = \"\"\"\nfunc.name(\n    arg1 + \n    1 +\n        2 + 3,\n    func(),\n    func(10000000000000),\n    arg2\n)\n\"\"\"\nast = to_ast(input_dax)\nassert ast is not None, \"AST should not be None\"\nsection = ast.args[0].right.left  # the \"1\" in \"arg1 + 1 + 2 + 3\"\nhighlighted = highlight_section(section)\nprint(highlighted.to_console())\n\n# Output:\n# 1 | func.name(\n# 2 |     arg1 +\n# 3 |       1 +\n#         ^\n# 4 |         2 + 3,\n# 5 |     func(),\n</code></pre> <p>Highlighting a larger section:</p> <pre><code>from pbi_parsers.dax import highlight_section, to_ast\n\ninput_dax = \"\"\"\nfunc.name(\n    arg1 + \n    1 +\n        2 + 3,\n    func(),\n    func(10000000000000),\n    arg2\n)\n\"\"\"\nast = to_ast(input_dax)\nassert ast is not None, \"AST should not be None\"\nsection = ast.args[0].right # The \"1 + 2\" in \"arg1 + 1 + 2 + 3\"\nhighlighted = highlight_section(section)\nprint(highlighted.to_console())\n# Output:\n# 1 | func.name(\n# 2 |     arg1 +\n# 3 |       1 +\n#         ^^^\n# 4 |         2 + 3,\n# ^^^^^^^^^^^^^^^^^\n# 5 |     func(),\n# 6 |     func(10000000000000),\n</code></pre>"},{"location":"api/dax/formatter/","title":"Formatter","text":"<p>Formats a DAX expression into a standardized format.</p> Source code in <code>pbi_parsers/dax/formatter.py</code> <pre><code>class Formatter:\n    \"\"\"Formats a DAX expression into a standardized format.\"\"\"\n\n    def __init__(self, expression: \"Expression\") -&gt; None:\n        self.expression = expression\n\n    def format(self) -&gt; str:\n        return self._format_helper(self.expression)\n\n    @classmethod\n    def _format_helper(cls, expr: Expression) -&gt; str:\n        mapper: Any = {\n            AddSubExpression: cls._format_add_sub,\n            AddSubUnaryExpression: cls._format_add_sub_unary,\n            ArrayExpression: cls._format_array,\n            ComparisonExpression: cls._format_comparison,\n            ColumnExpression: cls._format_column,\n            ConcatenationExpression: cls._format_concatenation,\n            DivMulExpression: cls._format_div_mul,\n            ExponentExpression: cls._format_exponent,\n            FunctionExpression: cls._format_function,\n            HierarchyExpression: cls._format_hierarchy,\n            IdentifierExpression: cls._format_identifier,\n            InExpression: cls._format_in,\n            KeywordExpression: cls._format_keyword,\n            LiteralNumberExpression: cls._format_literal_number,\n            LiteralStringExpression: cls._format_literal_string,\n            LogicalExpression: cls._format_logical,\n            MeasureExpression: cls._format_measure,\n            NoneExpression: lambda _: \"\",\n            ParenthesesExpression: cls._format_parens,\n            ReturnExpression: cls._format_return,\n            TableExpression: cls._format_table,\n            VariableExpression: cls._format_variable,\n        }\n        if type(expr) in mapper:\n            base_format = mapper[type(expr)](expr)\n            if expr.pre_comments:\n                base_format = f\"{format_comments(expr.pre_comments, 0)}\\n{base_format}\"\n            if expr.post_comments:\n                base_format = f\"{base_format}  {format_comments(expr.post_comments, 0)}\"\n            return base_format\n\n        msg = f\"Unsupported expression type: {type(expr).__name__}\"\n        raise TypeError(msg)\n\n    @classmethod\n    def _format_add_sub(cls, expr: AddSubExpression) -&gt; str:\n        left = cls._format_helper(expr.left)\n        right = cls._format_helper(expr.right)\n        return f\"\"\"{left} {expr.operator.text} {right}\"\"\"\n\n    @classmethod\n    def _format_add_sub_unary(cls, expr: AddSubUnaryExpression) -&gt; str:\n        return f\"{expr.operator.text}{cls._format_helper(expr.number)}\"\n\n    @classmethod\n    def _format_array(cls, expr: ArrayExpression) -&gt; str:\n        elements = \",\\n\".join(cls._format_helper(el) for el in expr.elements)\n        elements = textwrap.indent(elements, \" \" * 4)[4:]\n        return f\"\"\"{{\n    {elements}\n}}\n\"\"\"\n\n    @classmethod\n    def _format_comparison(cls, expr: ComparisonExpression) -&gt; str:\n        left = cls._format_helper(expr.left)\n        right = cls._format_helper(expr.right)\n        return f\"\"\"{left} {expr.operator.text} {right}\"\"\"\n\n    @classmethod\n    def _format_column(cls, expr: ColumnExpression) -&gt; str:\n        table = expr.table.text\n        if table.startswith(\"'\") and all(c in string.ascii_letters + string.digits + \"_\" for c in table[1:-1]):\n            table = table[1:-1]\n        column = expr.column.text\n        return f\"{table}{column}\"\n\n    @classmethod\n    def _format_concatenation(cls, expr: ConcatenationExpression) -&gt; str:\n        left = cls._format_helper(expr.left)\n        right = cls._format_helper(expr.right)\n        return f\"\"\"{left} {expr.operator.text} {right}\"\"\"\n\n    @classmethod\n    def _format_div_mul(cls, expr: DivMulExpression) -&gt; str:\n        left = cls._format_helper(expr.left)\n        right = cls._format_helper(expr.right)\n        return f\"\"\"{left} {expr.operator.text} {right}\"\"\"\n\n    @classmethod\n    def _format_exponent(cls, expr: ExponentExpression) -&gt; str:\n        base = cls._format_helper(expr.base)\n        power = cls._format_helper(expr.power)\n        return f\"\"\"{base}^{power}\"\"\"\n\n    @classmethod\n    def _format_function(cls, expr: FunctionExpression) -&gt; str:\n        name = \"\".join(token.text for token in expr.name_parts)\n        args = [cls._format_helper(arg) for arg in expr.args]\n        if sum(len(x) for x in args) &lt; MAX_ARGUMENT_LENGTH:\n            arg_str = \", \".join(args)\n            return f\"{name}({arg_str})\"\n        arg_str = textwrap.indent(\",\\n\".join(args), \" \" * 4)[4:]\n        return f\"\"\"\n{name}(\n    {arg_str}\n)\"\"\".strip()\n\n    @classmethod\n    def _format_hierarchy(cls, expr: HierarchyExpression) -&gt; str:\n        table = expr.table.text\n        if table.startswith(\"'\") and all(c in string.ascii_letters + string.digits + \"_\" for c in table[1:-1]):\n            table = table[1:-1]\n        return f\"{table}{expr.column.text}.{expr.level.text}\"\n\n    @classmethod\n    def _format_identifier(cls, expr: IdentifierExpression) -&gt; str:\n        return expr.name.text\n\n    @classmethod\n    def _format_in(cls, expr: InExpression) -&gt; str:\n        value = cls._format_helper(expr.value)\n        array = cls._format_helper(expr.array)\n        return f\"\"\"{value} IN {array}\"\"\"\n\n    @classmethod\n    def _format_keyword(cls, expr: KeywordExpression) -&gt; str:\n        return expr.name.text\n\n    @classmethod\n    def _format_literal_string(cls, expr: LiteralStringExpression) -&gt; str:\n        return expr.value.text\n\n    @classmethod\n    def _format_literal_number(cls, expr: LiteralNumberExpression) -&gt; str:\n        return expr.value.text\n\n    @classmethod\n    def _format_logical(cls, expr: LogicalExpression) -&gt; str:\n        left = cls._format_helper(expr.left)\n        right = cls._format_helper(expr.right)\n        return f\"\"\"{left} {expr.operator.text} {right}\"\"\"\n\n    @classmethod\n    def _format_measure(cls, expr: MeasureExpression) -&gt; str:\n        return expr.name.text\n\n    @classmethod\n    def _format_parens(cls, expr: ParenthesesExpression) -&gt; str:\n        inner = cls._format_helper(expr.inner_statement)\n        return f\"({inner})\"\n\n    @classmethod\n    def _format_return(cls, expr: ReturnExpression) -&gt; str:\n        variable_strs = \"\\n\".join(cls._format_helper(var) for var in expr.variable_statements)\n        return_statement: str = cls._format_helper(expr.ret)\n        return f\"\"\"\n{variable_strs}\nRETURN {return_statement}\n\"\"\".strip()\n\n    @classmethod\n    def _format_table(cls, expr: TableExpression) -&gt; str:\n        table_name = expr.name.text\n        if table_name.startswith(\"'\") and all(\n            c in string.ascii_letters + string.digits + \"_\" for c in table_name[1:-1]\n        ):\n            table_name = table_name[1:-1]\n        return table_name\n\n    @classmethod\n    def _format_variable(cls, expr: VariableExpression) -&gt; str:\n        return f\"{expr.var_name.text} = {cls._format_helper(expr.statement)}\"\n</code></pre>"},{"location":"api/dax/lexer/","title":"Lexer","text":"<p>               Bases: <code>BaseLexer</code></p> Source code in <code>pbi_parsers/dax/lexer.py</code> <pre><code>class Lexer(BaseLexer):\n    def scan(self) -&gt; tuple[Token]:\n        return super().scan()  # type: ignore[override]\n\n    def create_token(self, tok_type: TokenType, start_pos: int) -&gt; Token:\n        \"\"\"Create a new token with the given type and text.\"\"\"\n        text_slice = TextSlice(\n            full_text=self.source,\n            start=start_pos,\n            end=self.current_position,\n        )\n        return Token(tok_type=tok_type, text_slice=text_slice)\n\n    def _match_in(self, start_pos: int) -&gt; Token | None:\n        if self.match(\n            \"in \",\n            case_insensitive=True,\n        ):  # I have found no case where \"in\" is not followed by a space\n            # this allows us to avoid matching with the \"int\" function\n            self.advance(-1)  # leave the space to be consumed by whitespace handling\n            return self.create_token(\n                tok_type=TokenType.IN,\n                start_pos=start_pos,\n            )\n        return None\n\n    def _match_keyword(self, start_pos: int) -&gt; Token | None:\n        for keyword, token_type in KEYWORD_MAPPING.items():\n            if self.match(keyword, case_insensitive=True):\n                return self.create_token(\n                    tok_type=token_type,\n                    start_pos=start_pos,\n                )\n        return None\n\n    def _match_whitespace(self, start_pos: int) -&gt; Token | None:\n        if self.match(lambda c: c in WHITESPACE):\n            while self.match(lambda c: c in WHITESPACE):\n                pass\n            return self.create_token(\n                tok_type=TokenType.WHITESPACE,\n                start_pos=start_pos,\n            )\n        return None\n\n    def _match_var(self, start_pos: int) -&gt; Token | None:\n        if self.match(\"var\", case_insensitive=True):\n            return self.create_token(\n                tok_type=TokenType.VARIABLE,\n                start_pos=start_pos,\n            )\n        return None\n\n    def _match_return(self, start_pos: int) -&gt; Token | None:\n        if self.match(\"return\", case_insensitive=True):\n            return self.create_token(\n                tok_type=TokenType.RETURN,\n                start_pos=start_pos,\n            )\n        return None\n\n    def _match_period(self, start_pos: int) -&gt; Token | None:\n        if self.match(\".\"):\n            # must come before number literal to avoid conflict\n            return self.create_token(\n                tok_type=TokenType.PERIOD,\n                start_pos=start_pos,\n            )\n        return None\n\n    def _match_number_literal(self, start_pos: int) -&gt; Token | None:\n        if self.match(\n            lambda c: c.isdigit() or c == \".\",\n        ):  # must come before unquoted identifier to avoid conflict\n            while self.match(lambda c: c.isdigit() or c in {\".\", \"e\", \"E\"}):\n                pass\n            return self.create_token(\n                tok_type=TokenType.NUMBER_LITERAL,\n                start_pos=start_pos,\n            )\n        return None\n\n    def _match_unquoted_identifier(self, start_pos: int) -&gt; Token | None:\n        if self.match(lambda c: c.isalnum() or c == \"_\"):\n            while self.match(lambda c: c.isalnum() or c == \"_\"):\n                pass\n            return self.create_token(\n                tok_type=TokenType.UNQUOTED_IDENTIFIER,\n                start_pos=start_pos,\n            )\n        return None\n\n    def _match_single_quoted_identifier(self, start_pos: int) -&gt; Token | None:\n        if self.match(\"'\"):\n            while self.match(lambda c: c != \"'\"):\n                pass\n            if self.match(\"'\"):\n                return self.create_token(\n                    tok_type=TokenType.SINGLE_QUOTED_IDENTIFIER,\n                    start_pos=start_pos,\n                )\n            msg = \"Unterminated string literal\"\n            raise ValueError(msg)\n        return None\n\n    def _match_bracketed_identifier(self, start_pos: int) -&gt; Token | None:\n        if self.match(\"[\"):\n            while self.match(lambda c: c != \"]\"):\n                pass\n            if self.match(\"]\"):\n                return self.create_token(\n                    tok_type=TokenType.BRACKETED_IDENTIFIER,\n                    start_pos=start_pos,\n                )\n            msg = \"Unterminated bracketed identifier\"\n            raise ValueError(msg)\n        return None\n\n    def _match_string_literal(self, start_pos: int) -&gt; Token | None:\n        if self.match('\"'):\n            while self.match(lambda c: c != '\"') or self.match('\"\"'):\n                pass\n            if self.match('\"'):\n                return self.create_token(\n                    tok_type=TokenType.STRING_LITERAL,\n                    start_pos=start_pos,\n                )\n            msg = \"Unterminated string literal\"\n            raise ValueError(msg)\n        return None\n\n    def _match_single_line_comment(self, start_pos: int) -&gt; Token | None:\n        if self.match(\"//\") or self.match(\"--\"):\n            while self.match(lambda c: c not in {\"\\n\", \"\"}):\n                pass\n            return self.create_token(\n                tok_type=TokenType.SINGLE_LINE_COMMENT,\n                start_pos=start_pos,\n            )\n        return None\n\n    def _match_multi_line_comment(self, start_pos: int) -&gt; Token | None:\n        if not self.match(\"/*\"):\n            return None\n\n        while not self.at_end():\n            if self.match(\"*/\", chunk=2):\n                return self.create_token(\n                    tok_type=TokenType.MULTI_LINE_COMMENT,\n                    start_pos=start_pos,\n                )\n            self.advance()\n\n        msg = \"Unterminated multi-line comment\"\n        raise ValueError(msg)\n\n    def _match_token(self, start_pos: int) -&gt; Token | None:\n        fixed_character_mapping = {\n            \"(\": TokenType.LEFT_PAREN,\n            \")\": TokenType.RIGHT_PAREN,\n            \",\": TokenType.COMMA,\n            \"==\": TokenType.EQUAL_SIGN,\n            \"=\": TokenType.EQUAL_SIGN,\n            \"{\": TokenType.LEFT_CURLY_BRACE,\n            \"}\": TokenType.RIGHT_CURLY_BRACE,\n            \"&lt;&gt;\": TokenType.NOT_EQUAL_SIGN,\n            \"&lt;=\": TokenType.COMPARISON_OPERATOR,\n            \"&lt;\": TokenType.COMPARISON_OPERATOR,\n            \"&gt;=\": TokenType.COMPARISON_OPERATOR,\n            \"&gt;\": TokenType.COMPARISON_OPERATOR,\n            \"||\": TokenType.DOUBLE_PIPE_OPERATOR,\n            \"&amp;&amp;\": TokenType.DOUBLE_AMPERSAND_OPERATOR,\n            \"&amp;\": TokenType.AMPERSAND_OPERATOR,\n            \"+\": TokenType.PLUS_SIGN,\n            \"-\": TokenType.MINUS_SIGN,\n            \"^\": TokenType.EXPONENTIATION_SIGN,\n            \"*\": TokenType.MULTIPLY_SIGN,\n            \"%\": TokenType.MODULUS_SIGN,\n            \"/\": TokenType.DIVIDE_SIGN,\n        }\n\n        for char, token_type in fixed_character_mapping.items():\n            if self.match(char):\n                return self.create_token(tok_type=token_type, start_pos=start_pos)\n        return None\n\n    def scan_helper(self) -&gt; Token:\n        start_pos: int = self.current_position\n\n        if not self.peek():\n            return Token()\n\n        for candidate_func in (\n            self._match_in,\n            self._match_keyword,\n            self._match_whitespace,\n            self._match_var,\n            self._match_return,\n            self._match_period,\n            self._match_number_literal,\n            self._match_unquoted_identifier,\n            self._match_single_quoted_identifier,\n            self._match_bracketed_identifier,\n            self._match_string_literal,\n            self._match_single_line_comment,\n            self._match_multi_line_comment,\n            self._match_token,\n        ):\n            match_candidate = candidate_func(start_pos)\n            if match_candidate:\n                return match_candidate\n\n        msg = f\"Unexpected character: {self.peek()} at position {self.current_position}\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/dax/lexer/#pbi_parsers.dax.Lexer.create_token","title":"create_token","text":"<pre><code>create_token(tok_type: TokenType, start_pos: int) -&gt; Token\n</code></pre> <p>Create a new token with the given type and text.</p> Source code in <code>pbi_parsers/dax/lexer.py</code> <pre><code>def create_token(self, tok_type: TokenType, start_pos: int) -&gt; Token:\n    \"\"\"Create a new token with the given type and text.\"\"\"\n    text_slice = TextSlice(\n        full_text=self.source,\n        start=start_pos,\n        end=self.current_position,\n    )\n    return Token(tok_type=tok_type, text_slice=text_slice)\n</code></pre>"},{"location":"api/dax/parser/","title":"Parser","text":"Source code in <code>pbi_parsers/dax/parser.py</code> <pre><code>class Parser:\n    __tokens: list[Token]\n    index: int = 0\n    cache: dict[Any, Any]\n\n    def __init__(self, tokens: list[Token]) -&gt; None:\n        self.__tokens = tokens\n        self.index = 0\n        self.cache = {}\n\n    def peek(self, forward: int = 0) -&gt; Token:\n        \"\"\"Peek at the next token without advancing the index.\n\n        Args:\n            forward (int): How many tokens to look ahead. Defaults to 0.\n\n        Returns:\n            Token: The token at the current index + forward.\n\n        \"\"\"\n        if self.index + forward &gt;= len(self.__tokens):\n            return EOF_TOKEN\n        return self.__tokens[self.index + forward]\n\n    def remaining(self) -&gt; list[Token]:\n        \"\"\"Returns the remaining tokens from the current index.\n\n        Returns:\n            list[Token]: The list of tokens from the current index to the end.\n\n        \"\"\"\n        return self.__tokens[self.index :]\n\n    def to_ast(self) -&gt; \"Expression | None\":\n        \"\"\"Parse the tokens and return the root expression.\n\n        Raises:\n            ValueError: If no valid expression is found in the token stream.\n\n        \"\"\"\n        from .exprs import any_expression_match  # noqa: PLC0415\n\n        ret = any_expression_match(self)\n        if ret is None:\n            msg = \"No valid expression found in the token stream.\"\n            raise ValueError(msg)\n        assert self.peek().tok_type == TokenType.EOF\n        return ret\n\n    def consume(self) -&gt; Token:\n        \"\"\"Returns the next token and advances the index.\"\"\"\n        if self.index &gt;= len(self.__tokens):\n            return EOF_TOKEN\n        ret = self.__tokens[self.index]\n        self.index += 1\n        return ret\n</code></pre>"},{"location":"api/dax/parser/#pbi_parsers.dax.Parser.consume","title":"consume","text":"<pre><code>consume() -&gt; Token\n</code></pre> <p>Returns the next token and advances the index.</p> Source code in <code>pbi_parsers/dax/parser.py</code> <pre><code>def consume(self) -&gt; Token:\n    \"\"\"Returns the next token and advances the index.\"\"\"\n    if self.index &gt;= len(self.__tokens):\n        return EOF_TOKEN\n    ret = self.__tokens[self.index]\n    self.index += 1\n    return ret\n</code></pre>"},{"location":"api/dax/parser/#pbi_parsers.dax.Parser.peek","title":"peek","text":"<pre><code>peek(forward: int = 0) -&gt; Token\n</code></pre> <p>Peek at the next token without advancing the index.</p> <p>Parameters:</p> Name Type Description Default <code>forward</code> <code>int</code> <p>How many tokens to look ahead. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>Token</code> <code>Token</code> <p>The token at the current index + forward.</p> Source code in <code>pbi_parsers/dax/parser.py</code> <pre><code>def peek(self, forward: int = 0) -&gt; Token:\n    \"\"\"Peek at the next token without advancing the index.\n\n    Args:\n        forward (int): How many tokens to look ahead. Defaults to 0.\n\n    Returns:\n        Token: The token at the current index + forward.\n\n    \"\"\"\n    if self.index + forward &gt;= len(self.__tokens):\n        return EOF_TOKEN\n    return self.__tokens[self.index + forward]\n</code></pre>"},{"location":"api/dax/parser/#pbi_parsers.dax.Parser.remaining","title":"remaining","text":"<pre><code>remaining() -&gt; list[Token]\n</code></pre> <p>Returns the remaining tokens from the current index.</p> <p>Returns:</p> Type Description <code>list[Token]</code> <p>list[Token]: The list of tokens from the current index to the end.</p> Source code in <code>pbi_parsers/dax/parser.py</code> <pre><code>def remaining(self) -&gt; list[Token]:\n    \"\"\"Returns the remaining tokens from the current index.\n\n    Returns:\n        list[Token]: The list of tokens from the current index to the end.\n\n    \"\"\"\n    return self.__tokens[self.index :]\n</code></pre>"},{"location":"api/dax/parser/#pbi_parsers.dax.Parser.to_ast","title":"to_ast","text":"<pre><code>to_ast() -&gt; Expression | None\n</code></pre> <p>Parse the tokens and return the root expression.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid expression is found in the token stream.</p> Source code in <code>pbi_parsers/dax/parser.py</code> <pre><code>def to_ast(self) -&gt; \"Expression | None\":\n    \"\"\"Parse the tokens and return the root expression.\n\n    Raises:\n        ValueError: If no valid expression is found in the token stream.\n\n    \"\"\"\n    from .exprs import any_expression_match  # noqa: PLC0415\n\n    ret = any_expression_match(self)\n    if ret is None:\n        msg = \"No valid expression found in the token stream.\"\n        raise ValueError(msg)\n    assert self.peek().tok_type == TokenType.EOF\n    return ret\n</code></pre>"},{"location":"api/pq/formatter/","title":"Formatter","text":"Source code in <code>pbi_parsers/pq/formatter.py</code> <pre><code>class Formatter:\n    expression: Expression\n\n    def __init__(self, expression: Expression) -&gt; None:\n        self.expression = expression\n\n    def format(self) -&gt; str:\n        # Implement the formatting logic here\n        msg = \"Formatter.format method is not implemented.\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"api/pq/lexer/","title":"Lexer","text":"<p>               Bases: <code>BaseLexer</code></p> Source code in <code>pbi_parsers/pq/lexer.py</code> <pre><code>class Lexer(BaseLexer):\n    def scan(self) -&gt; tuple[Token]:\n        return super().scan()  # type: ignore[override]\n\n    def create_token(self, tok_type: TokenType, start_pos: int) -&gt; Token:\n        \"\"\"Create a new token with the given type and text.\"\"\"\n        text_slice = TextSlice(\n            full_text=self.source,\n            start=start_pos,\n            end=self.current_position,\n        )\n        return Token(tok_type=tok_type, text_slice=text_slice)\n\n    def _match_type_literal(self, start_pos: int) -&gt; Token | None:\n        for c in (\"int64.type\", \"currency.type\"):\n            if self.match(c, case_insensitive=True):\n                return self.create_token(\n                    tok_type=TokenType.TYPE_LITERAL,\n                    start_pos=start_pos,\n                )\n        return None\n\n    def _match_reserved_words(self, start_pos: int) -&gt; Token | None:\n        for name, token_type in RESERVED_WORDS:\n            if self.match(name, case_insensitive=True):\n                if not self.peek().isalpha():\n                    return self.create_token(tok_type=token_type, start_pos=start_pos)\n                # if the next character is an alpha character, it is not a keyword\n                # but an identifier, so we need to backtrack\n                self.advance(-len(name))\n        return None\n\n    def _match_keyword(self, start_pos: int) -&gt; Token | None:\n        for keyword in KEYWORDS:\n            if self.match(keyword, case_insensitive=True):\n                return self.create_token(tok_type=TokenType.KEYWORD, start_pos=start_pos)\n        return None\n\n    def _match_hash_identifier(self, start_pos: int) -&gt; Token | None:\n        if self.match('#\"'):\n            while self.match(lambda c: c != '\"') or self.match('\"\"'):\n                pass\n            if self.match('\"'):\n                return self.create_token(\n                    tok_type=TokenType.HASH_IDENTIFIER,\n                    start_pos=start_pos,\n                )\n            msg = f\"Unterminated string literal at positions: {start_pos} to {self.current_position}\"\n            raise ValueError(msg)\n\n        if self.match(\"#\"):\n            while self.match(lambda c: c in string.ascii_letters + string.digits + \"_\"):\n                pass\n            return self.create_token(\n                tok_type=TokenType.HASH_IDENTIFIER,\n                start_pos=start_pos,\n            )\n\n        return None\n\n    def _match_string_literal(self, start_pos: int) -&gt; Token | None:\n        if self.match('\"'):\n            while self.match(lambda c: c != '\"') or self.match('\"\"'):\n                pass\n            if self.match('\"'):\n                return self.create_token(\n                    tok_type=TokenType.STRING_LITERAL,\n                    start_pos=start_pos,\n                )\n            msg = f\"Unterminated string literal at positions: {start_pos} to {self.current_position}\"\n            raise ValueError(msg)\n\n        return None\n\n    def _match_whitespace(self, start_pos: int) -&gt; Token | None:\n        if self.match(lambda c: c in WHITESPACE):\n            while self.match(lambda c: c in WHITESPACE):\n                pass\n            return self.create_token(\n                tok_type=TokenType.WHITESPACE,\n                start_pos=start_pos,\n            )\n        return None\n\n    def _match_ellipsis(self, start_pos: int) -&gt; Token | None:\n        if self.match(\"...\"):\n            return self.create_token(\n                tok_type=TokenType.ELLIPSIS,\n                start_pos=start_pos,\n            )\n        return None\n\n    def _match_period(self, start_pos: int) -&gt; Token | None:\n        if self.match(\".\"):\n            return self.create_token(\n                tok_type=TokenType.PERIOD,\n                start_pos=start_pos,\n            )\n        return None\n\n    def _match_number_literal(self, start_pos: int) -&gt; Token | None:\n        if self.match(\n            lambda c: c.isdigit() or c == \".\",\n        ):  # must come before unquoted identifier to avoid conflict\n            while self.match(lambda c: c.isdigit() or c in {\".\", \"e\", \"E\"}):\n                pass\n            return self.create_token(\n                tok_type=TokenType.NUMBER_LITERAL,\n                start_pos=start_pos,\n            )\n        return None\n\n    def _match_unquoted_identifier(self, start_pos: int) -&gt; Token | None:\n        if self.match(lambda c: c.isalnum() or c == \"_\"):\n            while self.match(lambda c: c.isalnum() or c == \"_\"):\n                pass\n            return self.create_token(\n                tok_type=TokenType.UNQUOTED_IDENTIFIER,\n                start_pos=start_pos,\n            )\n        return None\n\n    def _match_single_line_comment(self, start_pos: int) -&gt; Token | None:\n        if self.match(\"//\") or self.match(\"--\"):\n            while self.match(lambda c: c not in {\"\\n\", \"\"}):\n                pass\n            return self.create_token(\n                tok_type=TokenType.SINGLE_LINE_COMMENT,\n                start_pos=start_pos,\n            )\n        return None\n\n    def _match_token(self, start_pos: int) -&gt; Token | None:\n        fixed_character_mapping = {\n            \"=&gt;\": TokenType.LAMBDA_ARROW,\n            \"&gt;=\": TokenType.COMPARISON_OPERATOR,\n            \"=\": TokenType.EQUAL_SIGN,\n            \"(\": TokenType.LEFT_PAREN,\n            \")\": TokenType.RIGHT_PAREN,\n            \"{\": TokenType.LEFT_CURLY_BRACE,\n            \"}\": TokenType.RIGHT_CURLY_BRACE,\n            \",\": TokenType.COMMA,\n            \"[\": TokenType.LEFT_BRACKET,\n            \"]\": TokenType.RIGHT_BRACKET,\n            \"&lt;&gt;\": TokenType.NOT_EQUAL_SIGN,\n            \"+\": TokenType.PLUS_SIGN,\n            \"-\": TokenType.MINUS_SIGN,\n            \"*\": TokenType.MULTIPLY_SIGN,\n            \"/\": TokenType.DIVIDE_SIGN,\n            \"&gt;\": TokenType.COMPARISON_OPERATOR,\n            \"&amp;\": TokenType.CONCATENATION_OPERATOR,\n            \"!\": TokenType.EXCLAMATION_POINT,\n        }\n\n        for char, token_type in fixed_character_mapping.items():\n            if self.match(char):\n                return self.create_token(\n                    tok_type=token_type,\n                    start_pos=start_pos,\n                )\n        return None\n\n    def scan_helper(self) -&gt; Token:\n        start_pos: int = self.current_position\n\n        if not self.peek():\n            return Token()\n\n        for candidate_func in (\n            self._match_type_literal,\n            self._match_reserved_words,\n            # keywords have to be checked after the above tokens because \"null\" blocks \"nullable\"\n            self._match_keyword,\n            self._match_hash_identifier,\n            self._match_string_literal,\n            self._match_whitespace,\n            self._match_ellipsis,\n            self._match_period,\n            self._match_number_literal,\n            self._match_unquoted_identifier,\n            self._match_hash_identifier,\n            self._match_single_line_comment,\n            self._match_token,\n        ):\n            match_candidate = candidate_func(start_pos)\n            if match_candidate:\n                return match_candidate\n\n        msg = f\"Unexpected character '{self.peek()}' at position {self.current_position}\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/pq/lexer/#pbi_parsers.pq.Lexer.create_token","title":"create_token","text":"<pre><code>create_token(tok_type: TokenType, start_pos: int) -&gt; Token\n</code></pre> <p>Create a new token with the given type and text.</p> Source code in <code>pbi_parsers/pq/lexer.py</code> <pre><code>def create_token(self, tok_type: TokenType, start_pos: int) -&gt; Token:\n    \"\"\"Create a new token with the given type and text.\"\"\"\n    text_slice = TextSlice(\n        full_text=self.source,\n        start=start_pos,\n        end=self.current_position,\n    )\n    return Token(tok_type=tok_type, text_slice=text_slice)\n</code></pre>"},{"location":"api/pq/parser/","title":"Parser","text":"Source code in <code>pbi_parsers/pq/parser.py</code> <pre><code>class Parser:\n    __tokens: list[Token]\n    index: int = 0\n    cache: dict[Any, Any]\n\n    def __init__(self, tokens: list[Token]) -&gt; None:\n        self.__tokens = tokens\n        self.index = 0\n        self.cache = {}\n\n    def peek(self, forward: int = 0) -&gt; Token:\n        \"\"\"Peek at the next token without advancing the index.\n\n        Args:\n            forward (int): How many tokens to look ahead. Defaults to 0.\n\n        Returns:\n            Token: The token at the current index + forward.\n\n        \"\"\"\n        if self.index + forward &gt;= len(self.__tokens):\n            return EOF_TOKEN\n        return self.__tokens[self.index + forward]\n\n    def remaining(self) -&gt; list[Token]:\n        \"\"\"Returns the remaining tokens from the current index.\n\n        Returns:\n            list[Token]: The list of tokens from the current index to the end.\n\n        \"\"\"\n        return self.__tokens[self.index :]\n\n    def to_ast(self) -&gt; \"Expression | None\":\n        \"\"\"Parse the tokens and return the root expression.\n\n        Raises:\n            ValueError: If no valid expression is found in the token stream.\n\n        \"\"\"\n        from .exprs import any_expression_match  # noqa: PLC0415\n\n        ret = any_expression_match(self)\n        if ret is None:\n            msg = \"No valid expression found in the token stream.\"\n            raise ValueError(msg)\n        assert self.peek().tok_type == TokenType.EOF\n        return ret\n\n    def consume(self) -&gt; Token:\n        \"\"\"Returns the next token and advances the index.\"\"\"\n        if self.index &gt;= len(self.__tokens):\n            return EOF_TOKEN\n        ret = self.__tokens[self.index]\n        self.index += 1\n        return ret\n</code></pre>"},{"location":"api/pq/parser/#pbi_parsers.pq.Parser.consume","title":"consume","text":"<pre><code>consume() -&gt; Token\n</code></pre> <p>Returns the next token and advances the index.</p> Source code in <code>pbi_parsers/pq/parser.py</code> <pre><code>def consume(self) -&gt; Token:\n    \"\"\"Returns the next token and advances the index.\"\"\"\n    if self.index &gt;= len(self.__tokens):\n        return EOF_TOKEN\n    ret = self.__tokens[self.index]\n    self.index += 1\n    return ret\n</code></pre>"},{"location":"api/pq/parser/#pbi_parsers.pq.Parser.peek","title":"peek","text":"<pre><code>peek(forward: int = 0) -&gt; Token\n</code></pre> <p>Peek at the next token without advancing the index.</p> <p>Parameters:</p> Name Type Description Default <code>forward</code> <code>int</code> <p>How many tokens to look ahead. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>Token</code> <code>Token</code> <p>The token at the current index + forward.</p> Source code in <code>pbi_parsers/pq/parser.py</code> <pre><code>def peek(self, forward: int = 0) -&gt; Token:\n    \"\"\"Peek at the next token without advancing the index.\n\n    Args:\n        forward (int): How many tokens to look ahead. Defaults to 0.\n\n    Returns:\n        Token: The token at the current index + forward.\n\n    \"\"\"\n    if self.index + forward &gt;= len(self.__tokens):\n        return EOF_TOKEN\n    return self.__tokens[self.index + forward]\n</code></pre>"},{"location":"api/pq/parser/#pbi_parsers.pq.Parser.remaining","title":"remaining","text":"<pre><code>remaining() -&gt; list[Token]\n</code></pre> <p>Returns the remaining tokens from the current index.</p> <p>Returns:</p> Type Description <code>list[Token]</code> <p>list[Token]: The list of tokens from the current index to the end.</p> Source code in <code>pbi_parsers/pq/parser.py</code> <pre><code>def remaining(self) -&gt; list[Token]:\n    \"\"\"Returns the remaining tokens from the current index.\n\n    Returns:\n        list[Token]: The list of tokens from the current index to the end.\n\n    \"\"\"\n    return self.__tokens[self.index :]\n</code></pre>"},{"location":"api/pq/parser/#pbi_parsers.pq.Parser.to_ast","title":"to_ast","text":"<pre><code>to_ast() -&gt; Expression | None\n</code></pre> <p>Parse the tokens and return the root expression.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid expression is found in the token stream.</p> Source code in <code>pbi_parsers/pq/parser.py</code> <pre><code>def to_ast(self) -&gt; \"Expression | None\":\n    \"\"\"Parse the tokens and return the root expression.\n\n    Raises:\n        ValueError: If no valid expression is found in the token stream.\n\n    \"\"\"\n    from .exprs import any_expression_match  # noqa: PLC0415\n\n    ret = any_expression_match(self)\n    if ret is None:\n        msg = \"No valid expression found in the token stream.\"\n        raise ValueError(msg)\n    assert self.peek().tok_type == TokenType.EOF\n    return ret\n</code></pre>"},{"location":"api/shared/lexer/","title":"Lexer","text":"Source code in <code>pbi_parsers/base/lexer.py</code> <pre><code>class BaseLexer:\n    source: str\n    start_position: int\n    current_position: int\n    tokens: list[BaseToken]\n\n    def __init__(self, source: str) -&gt; None:\n        self.source = source\n        self.start_position = 0\n        self.current_position = 0\n        self.tokens = []\n\n    def scan_helper(self) -&gt; BaseToken:\n        \"\"\"Contains the orchestration logic for converting tokens into expressions.\"\"\"\n        msg = \"Subclasses should implement match_tokens method.\"\n        raise NotImplementedError(msg)\n\n    def match(\n        self,\n        matcher: Callable[[str], bool] | str,\n        chunk: int = 1,\n        *,\n        case_insensitive: bool = True,\n    ) -&gt; bool:\n        \"\"\"Match a string or a callable matcher against the current position in the source.\n\n        Args:\n        ----\n            matcher (Callable[[str], bool] | str): A string to match or a callable that\n                takes a string and returns a boolean.\n            chunk (int): The number of characters to check from the current position.\n            case_insensitive (bool): If True, perform a case-insensitive match __only__ for strings.\n\n        \"\"\"\n        if isinstance(matcher, str):\n            chunk = len(matcher)\n\n        string_chunk = self.peek(chunk)\n        if not string_chunk:\n            return False\n\n        if isinstance(matcher, str):\n            if case_insensitive:\n                string_chunk = string_chunk.lower()\n                matcher = matcher.lower()\n            if string_chunk == matcher:\n                self.advance(chunk)\n                return True\n            return False\n\n        if matcher(string_chunk):\n            self.advance(chunk)\n            return True\n        return False\n\n    def peek(self, chunk: int = 1) -&gt; str:\n        \"\"\"Returns the next chunk of text from the current position. Defaults to a single character.\n\n        Args:\n            chunk (int): The number of characters to return from the current position.\n\n        Returns:\n            str: The next chunk of text from the current position.\n\n        \"\"\"\n        return (\n            self.source[self.current_position : self.current_position + chunk]\n            if self.current_position &lt; len(self.source)\n            else \"\"\n        )\n\n    def remaining(self) -&gt; str:\n        \"\"\"Returns the remaining text from the current position to the end of the source.\n\n        Only used for testing and debugging purposes.\n\n        Returns:\n            str: The remaining text from the current position to the end of the source.\n\n        \"\"\"\n        return self.source[self.current_position :]\n\n    def advance(self, chunk: int = 1) -&gt; None:\n        \"\"\"Advances the current position by the specified chunk size.\n\n        Generally used alongside peek to consume characters.\n\n        Args:\n            chunk (int): The number of characters to advance the current position.\n\n        Raises:\n            ValueError: If the current position exceeds a predefined MAX_POSITION (1,000,000 characters).\n                This is to avoid errors with the lexer causing the process to hang\n\n        \"\"\"\n        if self.current_position &gt; MAX_POSITION:\n            msg = f\"Current position exceeds {MAX_POSITION:,} characters.\"\n            raise ValueError(msg)\n        self.current_position += chunk\n\n    def scan(self) -&gt; tuple[BaseToken, ...]:\n        \"\"\"Repeatedly calls scan_helper until the end of the source is reached.\n\n        Returns:\n            tuple[BaseToken, ...]: A tuple of tokens scanned from the source.\n\n        \"\"\"\n        while not self.at_end():\n            self.tokens.append(self.scan_helper())\n        return tuple(self.tokens)\n\n    def at_end(self) -&gt; bool:\n        \"\"\"Checks if the current position is at (or beyond) the end of the source.\n\n        Returns:\n            bool: True if the current position is at or beyond the end of the source, False\n                otherwise.\n\n        \"\"\"\n        return self.current_position &gt;= len(self.source)\n</code></pre>"},{"location":"api/shared/lexer/#pbi_parsers.base.BaseLexer.advance","title":"advance","text":"<pre><code>advance(chunk: int = 1) -&gt; None\n</code></pre> <p>Advances the current position by the specified chunk size.</p> <p>Generally used alongside peek to consume characters.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>int</code> <p>The number of characters to advance the current position.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the current position exceeds a predefined MAX_POSITION (1,000,000 characters). This is to avoid errors with the lexer causing the process to hang</p> Source code in <code>pbi_parsers/base/lexer.py</code> <pre><code>def advance(self, chunk: int = 1) -&gt; None:\n    \"\"\"Advances the current position by the specified chunk size.\n\n    Generally used alongside peek to consume characters.\n\n    Args:\n        chunk (int): The number of characters to advance the current position.\n\n    Raises:\n        ValueError: If the current position exceeds a predefined MAX_POSITION (1,000,000 characters).\n            This is to avoid errors with the lexer causing the process to hang\n\n    \"\"\"\n    if self.current_position &gt; MAX_POSITION:\n        msg = f\"Current position exceeds {MAX_POSITION:,} characters.\"\n        raise ValueError(msg)\n    self.current_position += chunk\n</code></pre>"},{"location":"api/shared/lexer/#pbi_parsers.base.BaseLexer.at_end","title":"at_end","text":"<pre><code>at_end() -&gt; bool\n</code></pre> <p>Checks if the current position is at (or beyond) the end of the source.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the current position is at or beyond the end of the source, False otherwise.</p> Source code in <code>pbi_parsers/base/lexer.py</code> <pre><code>def at_end(self) -&gt; bool:\n    \"\"\"Checks if the current position is at (or beyond) the end of the source.\n\n    Returns:\n        bool: True if the current position is at or beyond the end of the source, False\n            otherwise.\n\n    \"\"\"\n    return self.current_position &gt;= len(self.source)\n</code></pre>"},{"location":"api/shared/lexer/#pbi_parsers.base.BaseLexer.match","title":"match","text":"<pre><code>match(matcher: Callable[[str], bool] | str, chunk: int = 1, *, case_insensitive: bool = True) -&gt; bool\n</code></pre> <p>Match a string or a callable matcher against the current position in the source.</p> <pre><code>matcher (Callable[[str], bool] | str): A string to match or a callable that\n    takes a string and returns a boolean.\nchunk (int): The number of characters to check from the current position.\ncase_insensitive (bool): If True, perform a case-insensitive match __only__ for strings.\n</code></pre> Source code in <code>pbi_parsers/base/lexer.py</code> <pre><code>def match(\n    self,\n    matcher: Callable[[str], bool] | str,\n    chunk: int = 1,\n    *,\n    case_insensitive: bool = True,\n) -&gt; bool:\n    \"\"\"Match a string or a callable matcher against the current position in the source.\n\n    Args:\n    ----\n        matcher (Callable[[str], bool] | str): A string to match or a callable that\n            takes a string and returns a boolean.\n        chunk (int): The number of characters to check from the current position.\n        case_insensitive (bool): If True, perform a case-insensitive match __only__ for strings.\n\n    \"\"\"\n    if isinstance(matcher, str):\n        chunk = len(matcher)\n\n    string_chunk = self.peek(chunk)\n    if not string_chunk:\n        return False\n\n    if isinstance(matcher, str):\n        if case_insensitive:\n            string_chunk = string_chunk.lower()\n            matcher = matcher.lower()\n        if string_chunk == matcher:\n            self.advance(chunk)\n            return True\n        return False\n\n    if matcher(string_chunk):\n        self.advance(chunk)\n        return True\n    return False\n</code></pre>"},{"location":"api/shared/lexer/#pbi_parsers.base.BaseLexer.peek","title":"peek","text":"<pre><code>peek(chunk: int = 1) -&gt; str\n</code></pre> <p>Returns the next chunk of text from the current position. Defaults to a single character.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>int</code> <p>The number of characters to return from the current position.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The next chunk of text from the current position.</p> Source code in <code>pbi_parsers/base/lexer.py</code> <pre><code>def peek(self, chunk: int = 1) -&gt; str:\n    \"\"\"Returns the next chunk of text from the current position. Defaults to a single character.\n\n    Args:\n        chunk (int): The number of characters to return from the current position.\n\n    Returns:\n        str: The next chunk of text from the current position.\n\n    \"\"\"\n    return (\n        self.source[self.current_position : self.current_position + chunk]\n        if self.current_position &lt; len(self.source)\n        else \"\"\n    )\n</code></pre>"},{"location":"api/shared/lexer/#pbi_parsers.base.BaseLexer.remaining","title":"remaining","text":"<pre><code>remaining() -&gt; str\n</code></pre> <p>Returns the remaining text from the current position to the end of the source.</p> <p>Only used for testing and debugging purposes.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The remaining text from the current position to the end of the source.</p> Source code in <code>pbi_parsers/base/lexer.py</code> <pre><code>def remaining(self) -&gt; str:\n    \"\"\"Returns the remaining text from the current position to the end of the source.\n\n    Only used for testing and debugging purposes.\n\n    Returns:\n        str: The remaining text from the current position to the end of the source.\n\n    \"\"\"\n    return self.source[self.current_position :]\n</code></pre>"},{"location":"api/shared/lexer/#pbi_parsers.base.BaseLexer.scan","title":"scan","text":"<pre><code>scan() -&gt; tuple[BaseToken, ...]\n</code></pre> <p>Repeatedly calls scan_helper until the end of the source is reached.</p> <p>Returns:</p> Type Description <code>tuple[BaseToken, ...]</code> <p>tuple[BaseToken, ...]: A tuple of tokens scanned from the source.</p> Source code in <code>pbi_parsers/base/lexer.py</code> <pre><code>def scan(self) -&gt; tuple[BaseToken, ...]:\n    \"\"\"Repeatedly calls scan_helper until the end of the source is reached.\n\n    Returns:\n        tuple[BaseToken, ...]: A tuple of tokens scanned from the source.\n\n    \"\"\"\n    while not self.at_end():\n        self.tokens.append(self.scan_helper())\n    return tuple(self.tokens)\n</code></pre>"},{"location":"api/shared/lexer/#pbi_parsers.base.BaseLexer.scan_helper","title":"scan_helper","text":"<pre><code>scan_helper() -&gt; BaseToken\n</code></pre> <p>Contains the orchestration logic for converting tokens into expressions.</p> Source code in <code>pbi_parsers/base/lexer.py</code> <pre><code>def scan_helper(self) -&gt; BaseToken:\n    \"\"\"Contains the orchestration logic for converting tokens into expressions.\"\"\"\n    msg = \"Subclasses should implement match_tokens method.\"\n    raise NotImplementedError(msg)\n</code></pre>"},{"location":"api/shared/text_slice/","title":"Text Slice","text":"Source code in <code>pbi_parsers/base/tokens.py</code> <pre><code>@dataclass\nclass TextSlice:\n    full_text: str = \"\"\n    start: int = -1\n    end: int = -1\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Checks equality based on the text slice.\"\"\"\n        if not isinstance(other, TextSlice):\n            return NotImplemented\n        return self.full_text == other.full_text and self.start == other.start and self.end == other.end\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Returns a hash based on the text slice.\"\"\"\n        return hash((self.full_text, self.start, self.end))\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Returns a string representation of the TextSlice.\"\"\"\n        return f\"TextSlice(text='{self.get_text()}', start={self.start}, end={self.end})\"\n\n    def get_text(self) -&gt; str:\n        \"\"\"Returns the text slice.\"\"\"\n        return self.full_text[self.start : self.end]\n</code></pre>"},{"location":"api/shared/text_slice/#pbi_parsers.base.tokens.TextSlice.__eq__","title":"__eq__","text":"<pre><code>__eq__(other: object) -&gt; bool\n</code></pre> <p>Checks equality based on the text slice.</p> Source code in <code>pbi_parsers/base/tokens.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"Checks equality based on the text slice.\"\"\"\n    if not isinstance(other, TextSlice):\n        return NotImplemented\n    return self.full_text == other.full_text and self.start == other.start and self.end == other.end\n</code></pre>"},{"location":"api/shared/text_slice/#pbi_parsers.base.tokens.TextSlice.__hash__","title":"__hash__","text":"<pre><code>__hash__() -&gt; int\n</code></pre> <p>Returns a hash based on the text slice.</p> Source code in <code>pbi_parsers/base/tokens.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Returns a hash based on the text slice.\"\"\"\n    return hash((self.full_text, self.start, self.end))\n</code></pre>"},{"location":"api/shared/text_slice/#pbi_parsers.base.tokens.TextSlice.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Returns a string representation of the TextSlice.</p> Source code in <code>pbi_parsers/base/tokens.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Returns a string representation of the TextSlice.\"\"\"\n    return f\"TextSlice(text='{self.get_text()}', start={self.start}, end={self.end})\"\n</code></pre>"},{"location":"api/shared/text_slice/#pbi_parsers.base.tokens.TextSlice.get_text","title":"get_text","text":"<pre><code>get_text() -&gt; str\n</code></pre> <p>Returns the text slice.</p> Source code in <code>pbi_parsers/base/tokens.py</code> <pre><code>def get_text(self) -&gt; str:\n    \"\"\"Returns the text slice.\"\"\"\n    return self.full_text[self.start : self.end]\n</code></pre>"},{"location":"api/shared/token/","title":"Token","text":"Source code in <code>pbi_parsers/base/tokens.py</code> <pre><code>@dataclass\nclass BaseToken:\n    tok_type: Any\n    text_slice: TextSlice = field(default_factory=TextSlice)\n\n    def __repr__(self) -&gt; str:\n        pretty_text = self.text_slice.get_text().replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\\\\r\")\n        return f\"Token(type={self.tok_type.name}, text='{pretty_text}')\"\n\n    @property\n    def text(self) -&gt; str:\n        \"\"\"Returns the text underlying the token.\"\"\"\n        return self.text_slice.get_text()\n\n    def position(self) -&gt; tuple[int, int]:\n        \"\"\"Returns the start and end positions of the token.\n\n        Returns:\n            tuple[int, int]: A tuple containing the start and end positions of the token within the source text.\n\n        \"\"\"\n        return self.text_slice.start, self.text_slice.end\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Checks equality based on token type and text slice.\"\"\"\n        if not isinstance(other, BaseToken):\n            return NotImplemented\n        return self.tok_type == other.tok_type and self.text_slice == other.text_slice\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Returns a hash based on token type and text slice.\"\"\"\n        return hash((self.tok_type, self.text_slice))\n</code></pre>"},{"location":"api/shared/token/#pbi_parsers.base.BaseToken.text","title":"text  <code>property</code>","text":"<pre><code>text: str\n</code></pre> <p>Returns the text underlying the token.</p>"},{"location":"api/shared/token/#pbi_parsers.base.BaseToken.__eq__","title":"__eq__","text":"<pre><code>__eq__(other: object) -&gt; bool\n</code></pre> <p>Checks equality based on token type and text slice.</p> Source code in <code>pbi_parsers/base/tokens.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"Checks equality based on token type and text slice.\"\"\"\n    if not isinstance(other, BaseToken):\n        return NotImplemented\n    return self.tok_type == other.tok_type and self.text_slice == other.text_slice\n</code></pre>"},{"location":"api/shared/token/#pbi_parsers.base.BaseToken.__hash__","title":"__hash__","text":"<pre><code>__hash__() -&gt; int\n</code></pre> <p>Returns a hash based on token type and text slice.</p> Source code in <code>pbi_parsers/base/tokens.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Returns a hash based on token type and text slice.\"\"\"\n    return hash((self.tok_type, self.text_slice))\n</code></pre>"},{"location":"api/shared/token/#pbi_parsers.base.BaseToken.position","title":"position","text":"<pre><code>position() -&gt; tuple[int, int]\n</code></pre> <p>Returns the start and end positions of the token.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>tuple[int, int]: A tuple containing the start and end positions of the token within the source text.</p> Source code in <code>pbi_parsers/base/tokens.py</code> <pre><code>def position(self) -&gt; tuple[int, int]:\n    \"\"\"Returns the start and end positions of the token.\n\n    Returns:\n        tuple[int, int]: A tuple containing the start and end positions of the token within the source text.\n\n    \"\"\"\n    return self.text_slice.start, self.text_slice.end\n</code></pre>"}]}